import numpy as np

# Initialize dataset and parameters
X = np.array([[1, 2], [2, 0], [3, 5]])
y = np.array([5, 4, 10])
w = np.array([0.5, 0.5])
b = 0.5
lambda_ = 0  # No regularization

# Dimensions
m, n = X.shape  # m = 3, n = 2

# Initialize gradient variables
dj_dw = np.zeros(n)  # dj_dw = [0.0, 0.0] (array with zeros)
dj_db = 0.0  # dj_db = 0 (scalar)

# Compute gradients
for i in range(m):  # Loop over each training example
    # Compute prediction for the i-th training example
    y_pre_i = np.dot(X[i], w) + b
    # Return type: scalar, e.g., 2.0, 1.5, 5.5
    print(f"y_pre_i (example {i}):", y_pre_i)
    
    # Compute error for the i-th training example
    err = y_pre_i - y[i]
    # Return type: scalar, e.g., -3.0, -2.5, -4.5
    print(f"Error (example {i}):", err)
    
    # Update gradients
    for j in range(n):  # Loop over each feature
        dj_dw[j] += err * X[i, j]
        # Return type after update: array, e.g., [-3.0, -6.0], [-8.0, -6.0], [-21.0, -36.0]
        print(f"dj_dw (after example {i}, feature {j}):", dj_dw)
    
    dj_db += err
    # Return type after update: scalar, e.g., -3.0, -5.5, -10.0
    print(f"dj_db (after example {i}):", dj_db)

# Average gradients over all training examples
dj_dw /= m  # Return type: array, e.g., [-1.67, -12.33]
dj_db /= m  # Return type: scalar, e.g., -3.33

# Print final results
print("Gradient with respect to weights (dj_dw):", dj_dw)
print("Gradient with respect to bias (dj_db):", dj_db)
